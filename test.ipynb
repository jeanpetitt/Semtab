{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import wandb,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/jean_dev/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from path_data.push_data_on_hub import login_hub\n",
    "token = login_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YuoeXg7obovXQ2k3HTHGy1KKr\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "print(os.environ['WANDB_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jean_dev/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"587df204b80582582b8a43c1722b1e927469062e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mlabonne/mini-platypus\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.train_test_split(test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 990\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'output'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"bert-base-multilingual-cased\", is_decoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jean_dev/Bureau/Project/SemTab/venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    gradient_accumulation_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 990/990 [00:01<00:00, 954.21 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 431.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"instruction\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from editdistance import eval as eval_distance\n",
    "import requests\n",
    "def openUrl(query):\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"language\": \"en\",\n",
    "        \"format\": \"json\",\n",
    "        \"search\": query,\n",
    "        \"limit\": 99,\n",
    "\n",
    "    }\n",
    "    uri = \"\"\n",
    "    output = []\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        if \"search\" in response.json():\n",
    "            result = response.json()[\"search\"]\n",
    "            if len(result) == 1:\n",
    "                    uri = f\"http://www.wikidata.org/entity/{result[0]['id']}\"\n",
    "                    output.append(\n",
    "                    f\" - Input : {query} \\n\\t-> id = {result[0]['id']}, link ={uri} \")\n",
    "                    print(output)\n",
    "                    return uri\n",
    "            elif len(result) > 2:\n",
    "                best_result = sorted(result, key=lambda r: eval_distance(query, r['label']))\n",
    "                result = f\"http://www.wikidata.org/entity/{best_result[0]['id']}\"\n",
    "                return result\n",
    "    except:\n",
    "        print(\"Internet connection error\")\n",
    "        return None\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai, os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_key = api_key\n",
    "def correct_spelling(text):\n",
    "        prompt = f\"Don't argument in your answer. Correct the spelling of this text : \\\"{text}\\\"\"\n",
    "        model = \"gpt-4o-mini\"\n",
    "        message_input = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0.8,\n",
    "            top_p=1,\n",
    "            seed=42,\n",
    "            messages=message_input,\n",
    "            max_tokens=256\n",
    "        )\n",
    "        \n",
    "        corrected_text = completion.choices[0].message.content.split('\"')\n",
    "        print(corrected_text)\n",
    "        if len(corrected_text) == 1:\n",
    "            corrected_text = \"\".join(corrected_text).strip()\n",
    "        elif len(corrected_text) <=3:\n",
    "            corrected_text = corrected_text[1]\n",
    "            corrected_text = f\"{corrected_text}.\".strip(\".\").strip()\n",
    "        else:\n",
    "            corrected_text = corrected_text[3].split(\".\")[0].strip()\n",
    "        return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The spelling is correct: ', 'Der Hauptmann von KÃ¶penick (The Captain of KÃ¶penick).', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Der Hauptmann von KÃ¶penick (The Captain of KÃ¶penick)'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = correct_spelling(\"Der Hauptmann von KÃ¶penick (The Captain of KÃ¶penick)\")\n",
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "\n",
    "def get_wikidata_label(wikidata_id):\n",
    "    \"\"\"\n",
    "    Get the label of a Wikidata entity by its QID.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{wikidata_id}.json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        entity = data['entities'][wikidata_id]\n",
    "        return entity['labels']['en']['value'] if 'en' in entity['labels'] else None\n",
    "    return None\n",
    "\n",
    "def get_wikidata_properties(wikidata_id):\n",
    "    \"\"\"\n",
    "    Get properties from a Wikidata entity.\n",
    "    :param wikidata_id: Wikidata ID (Qxxx)\n",
    "    :return: Dictionary of properties and their QID values\n",
    "    \"\"\"\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{wikidata_id}.json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        entity = data['entities'][wikidata_id]\n",
    "        claims = entity['claims']\n",
    "        properties = {}\n",
    "        \n",
    "        for prop, val in claims.items():\n",
    "            for claim in val:\n",
    "                if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
    "                    data_value = claim['mainsnak']['datavalue']\n",
    "                    if data_value['type'] == 'wikibase-entityid':\n",
    "                        qid = f\"Q{data_value['value']['numeric-id']}\"\n",
    "                        properties[prop] = qid\n",
    "        return properties\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_edit_distance_one(word1, word2):\n",
    "    # If the length difference is greater than 1, the edit distance can't be 1\n",
    "    if abs(len(word1) - len(word2)) > 1:\n",
    "        return False\n",
    "\n",
    "    # Initialize variables\n",
    "    len1, len2 = len(word1), len(word2)\n",
    "    i = j = 0\n",
    "    edit_count = 0\n",
    "\n",
    "    while i < len1 and j < len2:\n",
    "        if word1[i] != word2[j]:\n",
    "            # If there's more than one edit, return False\n",
    "            if edit_count == 1:\n",
    "                return False\n",
    "            edit_count += 1\n",
    "\n",
    "            # If the strings are of the same length, increment both pointers\n",
    "            if len1 == len2:\n",
    "                i += 1\n",
    "                j += 1\n",
    "            # If word1 is longer, increment the pointer for word1\n",
    "            elif len1 > len2:\n",
    "                i += 1\n",
    "            # If word2 is longer, increment the pointer for word2\n",
    "            else:\n",
    "                j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            j += 1\n",
    "\n",
    "    # If there's one more character left in either string\n",
    "    if i < len1 or j < len2:\n",
    "        edit_count += 1\n",
    "\n",
    "    return edit_count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_state = None\n",
    "current_state = None\n",
    "# Function to update the state and check for changes\n",
    "def state_has_changed(new_state):\n",
    "    global previous_state\n",
    "    global current_state\n",
    "\n",
    "    current_state = new_state\n",
    "    state_changed = False\n",
    "\n",
    "    if previous_state is not None and current_state != previous_state:\n",
    "        state_changed = True\n",
    "    \n",
    "    # Update the previous state for the next check\n",
    "    previous_state = current_state\n",
    "\n",
    "    return state_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(state_has_changed(\"OF\"))\n",
    "print(state_has_changed(\"ON\"))\n",
    "print(state_has_changed(\"ON\"))\n",
    "print(state_has_changed(\"OF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Process the CSV file and update Wikidata IDs.\n",
    "    :param input_file: Path to the input CSV file\n",
    "    :param output_file: Path to the output CSV file with updated Wikidata IDs\n",
    "    \"\"\"\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        header = next(reader)\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        current_wikidata_id = None\n",
    "        current_properties = {}\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            col_id = int(row[2])  # Third column is crucial (index 2)\n",
    "            label = row[3]  # Fourth column is the label (index 3)\n",
    "            # identifier = ''\n",
    "            \n",
    "            if col_id == 0:\n",
    "                identifier = str(row[5]).split(\"/\")[-1]  # Sixth column is the Wikidata ID (index 5)\n",
    "                # current_properties = get_wikidata_properties(current_wikidata_id)\n",
    "                # print(col_id, identifier)\n",
    "            else:\n",
    "                if not state_has_changed(identifier):     \n",
    "                    current_properties = get_wikidata_properties(identifier)\n",
    "                    if not current_properties:\n",
    "                        context = eval(row[4])\n",
    "                        id = openUrl(str(context[0]).strip(\".\").strip())\n",
    "                        # print(id)\n",
    "                        identifier = id.split(\"/\")[-1]\n",
    "                        current_properties = get_wikidata_properties(identifier)\n",
    "                        for prop, qid in current_properties.items():\n",
    "                            wikidata_label = get_wikidata_label(qid)\n",
    "                            # print(wikidata_label)\n",
    "                            if is_edit_distance_one(wikidata_label, label):\n",
    "                                row[5] = f\"http://www.wikidata.org/entity/{qid}\"  # Assign the correct Wikidata ID\n",
    "                                break\n",
    "                    else:\n",
    "                        for prop, qid in current_properties.items():\n",
    "                            wikidata_label = get_wikidata_label(qid)\n",
    "                            # print(wikidata_label)\n",
    "                            if is_edit_distance_one(wikidata_label, label):\n",
    "                                row[5] = f\"http://www.wikidata.org/entity/{qid}\"  # Assign the correct Wikidata ID\n",
    "                                break\n",
    "                else:\n",
    "                    current_properties = get_wikidata_properties(identifier)\n",
    "                    for prop, qid in current_properties.items():\n",
    "                        wikidata_label = get_wikidata_label(qid)\n",
    "                        # print(wikidata_label)\n",
    "                        if is_edit_distance_one(wikidata_label, label):\n",
    "                            row[5] = f\"http://www.wikidata.org/entity/{qid}\"  # Assign the correct Wikidata ID\n",
    "                            break\n",
    "            print(f\"cell {i}, label={label}, id={row[5]}\")\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_file('dataset/llm_test_cea_dataset.csv', 'output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
